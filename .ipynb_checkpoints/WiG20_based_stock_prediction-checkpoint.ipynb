{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRPxcxRLeO-3",
    "outputId": "7782bde1-2731-4777-dcd3-8a66ed4f7a86",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10388158626482253103\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 8855182400\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2881845996792935534\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rHrFTNJbAlBk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROqgQ2yGegGJ",
    "outputId": "5aa35344-42bc-4ffc-a7e3-6d89827b2b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: acp_d.csv\n",
      "Loaded file: ale_d.csv\n",
      "Loaded file: alr_d.csv\n",
      "Loaded file: ccc_d.csv\n",
      "Loaded file: cdr_d.csv\n",
      "Loaded file: cps_d.csv\n",
      "Loaded file: dnp_d.csv\n",
      "Loaded file: jsw_d.csv\n",
      "Loaded file: kgh_d.csv\n",
      "Loaded file: lpp_d.csv\n",
      "Loaded file: lts_d.csv\n",
      "Loaded file: opl_d.csv\n",
      "Loaded file: peo_d.csv\n",
      "Loaded file: pge_d.csv\n",
      "Loaded file: pgn_d.csv\n",
      "Loaded file: pkn_d.csv\n",
      "Loaded file: pko_d.csv\n",
      "Loaded file: pzu_d.csv\n",
      "Loaded file: san_d.csv\n",
      "Loaded file: tpe_d.csv\n"
     ]
    }
   ],
   "source": [
    "stock_list = ['Alior Bank', 'Allegro', 'Asseco', 'CCC', 'CD Projekt', 'Cyfrowy Polsat', 'Dino Polska', 'JSW', 'KGHM', 'Lotos', 'LPP', 'Orange Polska', 'PEKAO', 'PGE', 'PGNiG', 'PKN Orlen', 'PKO BP', 'PZU', 'Santander', 'Tauron']\n",
    "\n",
    "directory = \"WiG20 data/\"\n",
    "stock_data_list = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "    stock_data = stock_data.rename(columns={'Data': 'Date', 'Otwarcie': 'Open', 'Najwyzszy': 'Highest', 'Najnizszy': 'Lowest', 'Zamkniecie': 'Close', 'Wolumen': 'Volume'})\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data.Date)\n",
    "    stock_data_list.append(stock_data)\n",
    "    print('Loaded file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4hfNpIWSyMOU"
   },
   "outputs": [],
   "source": [
    "for i in range(len(stock_data_list)):\n",
    "    stock_data_list[i] = stock_data_list[i].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wym9bIyZ3sdA"
   },
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(data, rsi_period=10, so_period=14, so_d_period=4, tema_period=10, cgi_period=20, wpi_period=14):\n",
    "    # rsi_period - number of sessions considered when calculating RSI\n",
    "    # so_period - number of sessions considered when calculating stochastic oscillator K\n",
    "    # so_d_period - numbers of sessions considered when calculating moving average of the stochastic oscillator K\n",
    "    # tema_period - number of sessions considered when calculating TEMA\n",
    "    # cgi_period - number of sessions considered when calculating CGI\n",
    "    # wpi_period - number of sessions considered when calculating Williams' Percent Range\n",
    "\n",
    "    # Moving averages for periods of 10, 30 and 60 days\n",
    "    data['MovingAverage4'] = data['Close'].rolling(4).mean()\n",
    "    data['MovingAverage7'] = data['Close'].rolling(7).mean()\n",
    "    data['MovingAverage20'] = data['Close'].rolling(20).mean()\n",
    "\n",
    "    # Relative Strength Index RSI\n",
    "    increase_difference, decrease_difference = data['Close'].diff(), data['Close'].diff()\n",
    "    increase_difference[increase_difference < 0] = 0\n",
    "    decrease_difference[decrease_difference > 0] = 0\n",
    "    roll_increase = increase_difference.ewm(span = rsi_period).mean()\n",
    "    roll_decrease = decrease_difference.abs().ewm(span = rsi_period).mean()\n",
    "    RS = roll_increase / roll_decrease\n",
    "    data['RSI'] = 100 - (100 / (1 + RS))\n",
    "\n",
    "    # Rate of Change ROC\n",
    "    data['ROC'] = data['Close'].pct_change()\n",
    "\n",
    "    # Stochastic Oscillator K\n",
    "    L14, H14 = data['Close'].rolling(so_period).min(), data['Close'].rolling(so_period).max()\n",
    "    data['K'] = (data['Close'] - L14)/(H14 - L14)\n",
    "\n",
    "    # Moving average of the Stochastic Oscillator D\n",
    "    data['D'] = data['K'].rolling(so_d_period).mean()\n",
    "\n",
    "    # Moving Average Convergence / Divergence MACD\n",
    "    EMA_26 = data['Close'].ewm(26, adjust=False).mean()\n",
    "    EMA_12 = data['Close'].ewm(12, adjust=False).mean()\n",
    "    data['MACD'] = EMA_12 - EMA_26\n",
    "\n",
    "    # MACD Signal Line\n",
    "    data['MACD_Signal'] = data['MACD'].ewm(9, adjust=False).mean()\n",
    "\n",
    "    # MACD histogram\n",
    "    data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "\n",
    "    # Percentage Price Oscillator PPO\n",
    "    data['PPO'] =(EMA_12 - EMA_26)/EMA_26\n",
    "\n",
    "    # Triple Exponential Moving Average TEMA\n",
    "    SEMA = data['Close'].ewm(tema_period, adjust=False).mean()\n",
    "    DEMA = SEMA.ewm(tema_period, adjust=False).mean()\n",
    "    data['TEMA'] = DEMA.ewm(tema_period, adjust=False).mean()\n",
    "\n",
    "    # Commodity Channel Index CGI\n",
    "    typical_price = (data['Highest'] + data['Lowest'] + data['Close']) / 3\n",
    "    MA = typical_price.rolling(cgi_period).mean()\n",
    "    mean_deviation = (MA - typical_price).abs().rolling(cgi_period).mean()\n",
    "    data['CCI'] = (typical_price - MA) / (0.15 * mean_deviation)\n",
    "\n",
    "    # Williams' Percent Range\n",
    "    data['Percent_Range'] = (data['Highest'].rolling(wpi_period).max() - data['Close']) / (data['Highest'].rolling(wpi_period).max() - data['Lowest'].rolling(wpi_period).min())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SMbeHn6-hdjU"
   },
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    list_of_features = []\n",
    "    list_of_outputs = []\n",
    "    for column in data.columns:\n",
    "        list_of_features.append(data[column])\n",
    "        if column in ['Open', 'Close', 'Highest', 'Lowest', 'Volume', 'Otwarcie', 'Najwyzszy', 'Najnizszy', 'Zamkniecie', 'Wolumen']:\n",
    "            list_of_outputs.append(data[column])\n",
    "        \n",
    "    dataset = np.transpose(list_of_features)\n",
    "    output_dataset = np.transpose(list_of_outputs)\n",
    "    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = X_scaler.fit_transform(dataset)\n",
    "    Y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    Y_scaler.fit_transform(output_dataset)\n",
    "\n",
    "    return scaled_data, X_scaler, Y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D5KwQ3Jqt4qi"
   },
   "outputs": [],
   "source": [
    "def prepare_input_and_output(data, number_of_sessions=60):\n",
    "    # number_of_sessions - number of considered previous sessions as an input\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(number_of_sessions, data.shape[0]):\n",
    "        X.append(data[i-number_of_sessions:i, :])\n",
    "        Y.append(data[i, :5])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the results is made on closing prices - it gives the most information about the usefullness of the model\n",
    "\n",
    "def evaluate_results(X_valid, Y_valid, Y_scalers, predictions):\n",
    "\n",
    "    predictions = Y_scalers[0].inverse_transform(predictions)\n",
    "    real_output = Y_scalers[0].inverse_transform(Y_valid)\n",
    "\n",
    "    predicted_data = pd.DataFrame(predictions, columns=['Open_predicted', 'Close_predicted', 'Highest_predicted', 'Lowest_predicted', 'Volume_predicted'])\n",
    "    real_data = pd.DataFrame(real_output, columns=['Open_real', 'Close_real', 'Highest_real', 'Lowest_real', 'Volume_real'])\n",
    "    predictions = pd.concat([real_data, predicted_data], axis=1)\n",
    "    print(predictions[['Close_real', 'Close_predicted']].tail(20))\n",
    "\n",
    "    predictions['Close_difference'] = abs(predictions['Close_real'] - predictions['Close_predicted'])\n",
    "    predictions['Close_difference_percent'] = abs(predictions['Close_real'] - predictions['Close_predicted'])/predictions['Close_real'] * 100\n",
    "\n",
    "    previous_close = predictions['Close_real'].shift(-1)\n",
    "    Naive_forcast_MAPE = (abs(predictions['Close_real'] - previous_close)/predictions['Close_real'] * 100).mean()\n",
    "    predictions_MAPE = predictions['Close_difference_percent'].mean()\n",
    "\n",
    "    print('Naive forcast MAE: ' + str(round(abs(previous_close - predictions['Close_real']).mean(),2)))\n",
    "    print('Predictions MAE: ' + str(round(predictions['Close_difference'].mean(),2)))\n",
    "    print('Naive forcast MAPE: ' + str(round(Naive_forcast_MAPE,2)) + '%')\n",
    "    print('Predictions MAPE: ' + str(round(predictions_MAPE,2)) + '%')\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTwWMPg217MJ"
   },
   "source": [
    "# Lerning based on prices and volume only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wdozVb0r166b"
   },
   "outputs": [],
   "source": [
    "scaled_stocks_basic = []\n",
    "X_scalers_basic = []\n",
    "Y_scalers_basic = []\n",
    "\n",
    "for i in range(len(stock_data_list)):\n",
    "    # Date is dropped as it isn't considered for learning\n",
    "    scaled_stock, X_scaler, Y_scaler = scale_data(stock_data_list[i].drop(columns='Date'))\n",
    "  \n",
    "    scaled_stocks_basic.append(scaled_stock)\n",
    "    X_scalers_basic.append(X_scaler)\n",
    "    Y_scalers_basic.append(Y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cR5gBYy22ZUM",
    "outputId": "21a9fea4-878f-49bd-8821-e0e9fbce5aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input matrix: (76328, 60, 5)\n",
      "Shape of output matrix: (76328, 5)\n"
     ]
    }
   ],
   "source": [
    "X_all_basic = []\n",
    "Y_all_basic = []\n",
    "scaled_X_list_basic = []\n",
    "scaled_Y_list_basic = []\n",
    "\n",
    "for stock in scaled_stocks_basic:\n",
    "    X, Y = prepare_input_and_output(stock, 60)\n",
    "    scaled_X_list_basic.append(np.array(X))\n",
    "    scaled_Y_list_basic.append(np.array(Y))\n",
    "    X_all_basic = X_all_basic + X\n",
    "    Y_all_basic = Y_all_basic + Y\n",
    "\n",
    "X_all_basic, Y_all_basic = np.array(X_all_basic), np.array(Y_all_basic)\n",
    "print('Shape of input matrix: ' + str(X_all_basic.shape))\n",
    "print('Shape of output matrix: ' + str(Y_all_basic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpQC3Vld3Cky",
    "outputId": "21c85455-4fd7-404d-f133-f58f7933dc5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_basic, X_valid_basic, Y_train_basic, Y_valid_basic = train_test_split(X_all_basic, Y_all_basic, test_size=2667, shuffle=False)\n",
    "\n",
    ","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 883
    },
    "id": "GKgpalBO3MOB",
    "outputId": "19421dbe-d999-4565-ea03-4a4e318890f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                17920     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 24,925\n",
      "Trainable params: 24,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_2960]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7d1a018d6aa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel_basic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_basic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_basic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_basic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_basic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid_basic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\stock_prediction\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_2960]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model_basic = Sequential()\n",
    "model_basic.add(LSTM(64, input_shape=[X_train_basic.shape[1], X_train_basic.shape[2]]))#, return_sequences=True))\n",
    "#model_basic.add(LSTM(64))\n",
    "model_basic.add(Dense(100, activation='relu'))\n",
    "model_basic.add(Dense(Y_train_basic.shape[1]))\n",
    "\n",
    "model_basic.summary()\n",
    "\n",
    "model_basic.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history = model_basic.fit(X_train_basic, Y_train_basic, epochs=15, validation_data=(X_valid_basic, Y_valid_basic))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_basic = model_basic.predict(X_valid_basic)\n",
    "\n",
    "predictions_basic = evaluate_results(X_valid_basic, Y_valid_basic, Y_scalers_basic, predictions_basic) # returns dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Porównanie rzeczywistych i przewidywanych cen Tauron SA', fontsize=50)\n",
    "plt.xlabel('Nr sesji', fontsize=30)\n",
    "plt.ylabel('Cena zamknięcia (zł)', fontsize=30)\n",
    "plt.plot(predictions_basic['Close_real'])\n",
    "plt.plot(predictions_basic['Close_predicted'])\n",
    "plt.legend(['Cena rzeczywista',  'Cena przewidywana'], loc='lower right', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa15Drwo2GiE"
   },
   "source": [
    "# Lerning based on prices, volume and technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "GPA-Ze1S5lH-",
    "outputId": "481befc5-9cb4-43a2-e5ef-ece0ea2cb53a"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "stock_data_list_ta = []\n",
    "\n",
    "for i in range(len(stock_data_list)):\n",
    "    technical_indicators = calculate_technical_indicators(stock_data_list[i], rsi_period=60, so_period=5, so_d_period=3, tema_period=4, cgi_period=4, wpi_period=4)\n",
    "    stock_data_list_ta.append(technical_indicators)\n",
    "    # moving averages return NaN when the considered period is greater than available data, these rows need to be dropped\n",
    "    stock_data_list_ta[i] = stock_data_list_ta[i].dropna()\n",
    "\n",
    "display.display(stock_data_list_ta[0].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6SLv9Gn9J33"
   },
   "outputs": [],
   "source": [
    "scaled_stocks_ta = []\n",
    "X_scalers_ta = []\n",
    "Y_scalers_ta = []\n",
    "\n",
    "for i in range(len(stock_data_list_ta)):\n",
    "    # Date is dropped as it isn't considered for learning\n",
    "    scaled_stock_ta, X_scaler_ta, Y_scaler_ta = scale_data(stock_data_list_ta[i].drop(columns='Date'))\n",
    "    scaled_stocks_ta.append(scaled_stock_ta)\n",
    "    X_scalers_ta.append(X_scaler_ta)\n",
    "    Y_scalers_ta.append(Y_scaler_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcEx0942taj9",
    "outputId": "d75340a2-9821-470b-8374-d1795f00a034"
   },
   "outputs": [],
   "source": [
    "X_all_ta = []\n",
    "Y_all_ta = []\n",
    "scaled_X_list_ta = []\n",
    "scaled_Y_list_ta = []\n",
    "\n",
    "for stock in scaled_stocks_ta:\n",
    "    X, Y = prepare_input_and_output(stock)\n",
    "    scaled_X_list_ta.append(np.array(X))\n",
    "    scaled_Y_list_ta.append(np.array(Y))\n",
    "    X_all_ta = X_all_ta + X\n",
    "    Y_all_ta = Y_all_ta + Y\n",
    "\n",
    "X_all_ta, Y_all_ta = np.array(X_all_ta), np.array(Y_all_ta)\n",
    "print('Shape of input matrix: ' + str(X_all_ta.shape))\n",
    "print('Shape of output matrix: ' + str(Y_all_ta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3-GXVfNuo35",
    "outputId": "fb8890c3-9cc0-4ec0-f49b-ca83806c3be7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_ta, X_valid_ta, Y_train_ta, Y_valid_ta = train_test_split(X_all_ta, Y_all_ta, test_size=2667, shuffle=False)\n",
    "\n",
    "print('Shape of training input matrix: ' + str(X_train_ta.shape))\n",
    "print('Shape of training output matrix: ' + str(Y_train_ta.shape))\n",
    "print('Shape of validation input matrix: ' + str(X_valid_ta.shape))\n",
    "print('Shape of validation output matrix: ' + str(Y_valid_ta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kB4Cppg_Hep"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToVHaihd57qh",
    "outputId": "b02dcf45-25b3-47b6-e01f-a689ab8d3ee5"
   },
   "outputs": [],
   "source": [
    "model_ta = Sequential()\n",
    "model_ta.add(LSTM(64,input_shape=[X_train_ta.shape[1], X_train_ta.shape[2]]))#, return_sequences=True))\n",
    "model_ta.add(Dense(100, activation='relu'))\n",
    "model_ta.add(Dense(Y_train_ta.shape[1]))\n",
    "\n",
    "model_ta.summary()\n",
    "\n",
    "model_ta.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history_ta = model_ta.fit(X_train_ta, Y_train_ta, epochs=15, validation_data=(X_valid_ta, Y_valid_ta))\n",
    "\n",
    "plt.plot(history_ta.history['loss'], label='train')\n",
    "plt.plot(history_ta.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ta = model_ta.predict(X_valid_ta)\n",
    "\n",
    "predictions_ta = evaluate_results(X_valid_ta, Y_valid_ta, Y_scalers_ta, predictions_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Porównanie rzeczywistych i przewidywanych z użyciem \\nwskaźników technicznych cen Tauron SA', fontsize=50)\n",
    "plt.xlabel('Nr sesji', fontsize=30)\n",
    "plt.ylabel('Cena zamknięcia (zł)', fontsize=30)\n",
    "plt.plot(predictions_ta['Close_real'])\n",
    "plt.plot(predictions_ta['Close_predicted'])\n",
    "plt.legend(['Cena rzeczywista',  'Cena przewidywana'], loc='lower right', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning based on price, volume and fundamental indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_data = pd.read_csv('Testing_data/WiG20_full_data.csv')\n",
    "\n",
    "fundamental_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly we use stocks that have every fundamental indicator. Indicators like Wartość księgowa Grahama (one among many) don't apply to stocks 'ALR', 'PEO', 'PGN', 'PKO', 'PZU' so we drop them off**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_stocks_fa = []\n",
    "X_scalers_fa = []\n",
    "Y_scalers_fa = []\n",
    "\n",
    "for stock in list(set(fundamental_data['Spółka'].unique()) - set(['ALR', 'PEO', 'PGN', 'PKO', 'PZU', 'SPL'])) :\n",
    "    # Date, stock and quarters is dropped as it isn't considered for learning\n",
    "    scaled_stock, X_scaler, Y_scaler = scale_data(fundamental_data.loc[fundamental_data['Spółka'] == stock].drop(columns=['Spółka', 'Data', 'Kwartały']))\n",
    "  \n",
    "    scaled_stocks_fa.append(scaled_stock)\n",
    "    X_scalers_fa.append(X_scaler)\n",
    "    Y_scalers_fa.append(Y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_data['Spółka'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_fa = []\n",
    "Y_all_fa = []\n",
    "scaled_X_list_fa = []\n",
    "scaled_Y_list_fa = []\n",
    "\n",
    "for stock in scaled_stocks_fa:\n",
    "    X, Y = prepare_input_and_output(stock, 60)\n",
    "    scaled_X_list_fa.append(np.array(X))\n",
    "    scaled_Y_list_fa.append(np.array(Y))\n",
    "    X_all_fa = X_all_fa + X\n",
    "    Y_all_fa = Y_all_fa + Y\n",
    "\n",
    "X_all_fa, Y_all_fa = np.array(X_all_fa), np.array(Y_all_fa)\n",
    "print('Shape of input matrix: ' + str(X_all_fa.shape))\n",
    "print('Shape of output matrix: ' + str(Y_all_fa.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPE_records = fundamental_data.loc[fundamental_data['Spółka'] == 'TPE'].shape[0]\n",
    "print('TPE records: ' + str(TPE_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We use one of the stocks data for validation - the TPE which has 2486 records\n",
    "X_train_fa, X_valid_fa, Y_train_fa, Y_valid_fa = train_test_split(X_all_fa, Y_all_fa, test_size=TPE_records, shuffle=False)\n",
    "\n",
    "print('Shape of training input matrix: ' + str(X_train_fa.shape))\n",
    "print('Shape of training output matrix: ' + str(Y_train_fa.shape))\n",
    "print('Shape of validation input matrix: ' + str(X_valid_fa.shape))\n",
    "print('Shape of validation output matrix: ' + str(Y_valid_fa.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fa = Sequential()\n",
    "model_fa.add(LSTM(64, input_shape=[X_train_fa.shape[1], X_train_fa.shape[2]]))#, return_sequences=True))\n",
    "#model.add(LSTM(64))\n",
    "model_fa.add(Dense(100, activation='relu'))\n",
    "model_fa.add(Dense(Y_train_fa.shape[1]))\n",
    "\n",
    "model_fa.summary()\n",
    "\n",
    "model_fa.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "history_fa = model_fa.fit(X_train_fa, Y_train_fa, epochs=15, validation_data=(X_valid_fa, Y_valid_fa))\n",
    "\n",
    "plt.plot(history_fa.history['loss'], label='train')\n",
    "plt.plot(history_fa.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_fa = model_fa.predict(X_valid_fa)\n",
    "\n",
    "predictions_fa = evaluate_results(X_valid_fa, Y_valid_fa, Y_scalers_fa, predictions_fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "plt.title('Porównanie rzeczywistych i przewidywanych z użyciem \\nwskaźników fundamentalnych cen Tauron SA', fontsize=50)\n",
    "plt.xlabel('Nr sesji', fontsize=30)\n",
    "plt.ylabel('Cena zamknięcia (zł)', fontsize=30)\n",
    "plt.plot(predictions_fa['Close_real'])\n",
    "plt.plot(predictions_fa['Close_predicted'])\n",
    "plt.legend(['Cena rzeczywista',  'Cena przewidywana'], loc='lower right', fontsize=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WiG20 based stock prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
